{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcusLongton/Used_Cars_Analysis/blob/main/Car_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW1d0jsMvrK-"
      },
      "source": [
        "# What drives the price of a car?\n",
        "\n",
        "![](images/kurt.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L8BkPmNvrK_"
      },
      "source": [
        "**OVERVIEW**\n",
        "\n",
        "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJ3pWKIvrLA"
      },
      "source": [
        "### CRISP-DM Framework\n",
        "\n",
        "<center>\n",
        "    <img src = images/crisp.png width = 50%/>\n",
        "</center>\n",
        "\n",
        "\n",
        "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0zcIlW_vrLA"
      },
      "source": [
        "### Business Understanding\n",
        "\n",
        "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5et_lKYx4MK"
      },
      "source": [
        "###Our tast is to determine, what do customers value in a used car? Our answer will come in the form of the names of several features of our dataset that we conclude to hold the most significance or in other words, have the most correlation, with our target variable. We will go through several steps to solve this problem. The first is making sure we have a very solid understanding of the real question we are being asked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xT6V7rUvrLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohLs1PkOvrLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uXM6y4avrLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwqrBl2cvrLA"
      },
      "source": [
        "### Data Understanding\n",
        "\n",
        "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgrLr_qt7U87"
      },
      "source": [
        "Here will be our first look at the dataset itself. There will be several things that we will want to explore including, the structure of the dataset, what different datatypes it has, and missing values or poorly recored data it might contain. In addition, we will need to begin constructing some visualizations to aid us in our understanding of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5UnvE1avrLB"
      },
      "outputs": [],
      "source": [
        "# Begin by importing the libraries that we will be using\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aR0lQyLaHEW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72VDODxGvrLB"
      },
      "outputs": [],
      "source": [
        "# Reading our csv file into a pandas DataFrame\n",
        "filename = '/content/vehicles.csv'\n",
        "df = pd.read_csv(filename)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gibz9xQvrLB"
      },
      "outputs": [],
      "source": [
        "# Listing our data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQuLoe_aSXos"
      },
      "source": [
        "The above cell tells us that our data is mostly of a categorical type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfQm2lVAA2dB"
      },
      "outputs": [],
      "source": [
        "# Describe our data\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JSSJve4vrLB"
      },
      "outputs": [],
      "source": [
        "df.columns # Listing all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKiYSK39Su9"
      },
      "outputs": [],
      "source": [
        "df['price'].isna().value_counts() # Price will be the variable that wer will try and predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMLzbZmb9Sr-"
      },
      "outputs": [],
      "source": [
        "# Looking at how many missing values we have per feature\n",
        "missing_vals = df.isnull().sum().sort_values()\n",
        "print(missing_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ausb54B_m5vA"
      },
      "outputs": [],
      "source": [
        "# Count the number of null values in each row\n",
        "null_count_per_row = df.isnull().sum(axis=1)\n",
        "\n",
        "# Find rows with more than 5 null values\n",
        "rows_with_more_than_5_nulls = null_count_per_row[null_count_per_row > 5]\n",
        "\n",
        "# Get the number of such rows\n",
        "num_rows_with_more_than_5_nulls = rows_with_more_than_5_nulls.count()\n",
        "\n",
        "print(f'Number of rows with more than 5 null values: {num_rows_with_more_than_5_nulls}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S39fCoO69Son"
      },
      "outputs": [],
      "source": [
        "# Looking at the distribution of prices\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['price'], bins=30, kde=True)\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Price')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTvp6lb5Ee9Z"
      },
      "source": [
        "This initial Histogram provides more questions than answers about our dependent variable. I am hypothesizing that this feature might have some values of 0 as well as some high outliers that are skewing our histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnXCtm4t9Sk0"
      },
      "outputs": [],
      "source": [
        "df['price'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHUmBC429ShD"
      },
      "outputs": [],
      "source": [
        "(df['price'] == 0).sum() # Number of rows with price equal to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giFKHHV19SdZ"
      },
      "outputs": [],
      "source": [
        "df['price'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf4_EB8_9SaK"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the IQR (Interquartile Range)\n",
        "Q1 = df['price'].quantile(0.01)\n",
        "Q3 = df['price'].quantile(0.99)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Step 2: Filter out the outliers (values outside the range of Q1 - 1.5*IQR to Q3 + 1.5*IQR)\n",
        "df_no_outliers = df[(df['price'] >= (Q1 - 1.5 * IQR)) & (df['price'] <= (Q3 + 1.5 * IQR))]\n",
        "df_no_outliers_no_zeros = df_no_outliers[df_no_outliers['price'] > 1000] # Set this to be a small positive value.\n",
        "\n",
        "# Step 3: Plot the histogram without outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_no_outliers_no_zeros['price'], kde=True, bins=30)\n",
        "plt.title('Histogram of Price (Without Outliers)')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUjH4aOmRPz"
      },
      "source": [
        "In our Data Preparation phase we will certainly have to take out all of the zero values from 'price'. One of the biggest qestions will be how to deal with the shear quantity of null values we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr1ISszq9SWI"
      },
      "outputs": [],
      "source": [
        "df_numeric = df.select_dtypes(include=['number'])\n",
        "df_numeric.corr() # Looking at correlation of our numeric features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spWJgVEmrjDs"
      },
      "outputs": [],
      "source": [
        "# Step 1: Calculate the IQR (Interquartile Range)\n",
        "Q1 = df['price'].quantile(0.25)\n",
        "Q3 = df['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Step 2: Filter out the outliers (values outside the range of Q1 - 1.5*IQR to Q3 + 1.5*IQR)\n",
        "df_no_outliers = df[(df['price'] >= (Q1 - 1.5 * IQR)) & (df['price'] <= (Q3 + 1.5 * IQR))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O602Bkwfr6zl"
      },
      "outputs": [],
      "source": [
        "df['odometer'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvHIjN0n9SKT"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(df_no_outliers, x='year', y='price', title='Price vs Year', color='odometer', trendline='ols')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuUnyu-9rWeX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(12,6)) # Figure with 1 row and 2 columns\n",
        "\n",
        "sns.regplot(x='year', y='price', data=df_no_outliers, ax=ax[0], scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n",
        "ax[0].set_title('Price vs Year with Trendline')\n",
        "\n",
        "\n",
        "sns.histplot(df_no_outliers['price'], kde=True, ax=ax[1])\n",
        "ax[1].set_title('Distribution of Price')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPZyBtY2rWcD"
      },
      "outputs": [],
      "source": [
        "# Sorting by price\n",
        "df.sort_values(by='price', ascending=False).head(10) # Notice lots of values where the price is extremely high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOPJrBgbrWZL"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by='price', ascending=True) # Notice lots of Values where price is equal to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6wsV1pxyFqs"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 6))\n",
        "sns.regplot(x='year', y='price', data=df_no_outliers, scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1WnmtvKyFlN"
      },
      "outputs": [],
      "source": [
        "# Making a distribution of the numbers of each car condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOAnnwnSyFiL"
      },
      "outputs": [],
      "source": [
        "man_order = df['manufacturer'].value_counts().index # Sort to show manufacturer from most to least common\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "sns.countplot(x='manufacturer', data=df, order=man_order)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Distribution of Car Manufacturers')\n",
        "plt.xlabel('Manufacturer')\n",
        "plt.ylabel('Number of Cars')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTPHc6xQzqJH"
      },
      "outputs": [],
      "source": [
        "condition_order = df['condition'].value_counts().index # Sort to show condition from most to least common\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.countplot(x='condition', data=df, order=condition_order)\n",
        "plt.title('Distribution of Car Conditions')\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Number of Cars')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if they are long\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN4N9XKyyFe3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaG7lPI2vrLB"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdoapAJfkU98"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cXoTJRRvrLB"
      },
      "outputs": [],
      "source": [
        "# show me the number of cars whose price is less than 1000\n",
        "df[df['price'] < 500].value_counts().sum() # Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGkE8sbVvrLB"
      },
      "outputs": [],
      "source": [
        "df.query('price < 500 & price > 50').groupby('condition').size() # Shows us the number of cars between $50 and $500 based on their condition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6iT8wLfFwaC"
      },
      "source": [
        "Referring to the above cell. I do not think it makes sense that the vast majority of cars between the price of #500 and $50 fall into the Excellent, good, or like new categories. When I filter my dataframe. I will set 500 to my lower bound."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxJaYEeOgkcg"
      },
      "outputs": [],
      "source": [
        "print(df.select_dtypes(include=['number'])) # Display numeric features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuGrVGAtvrLB"
      },
      "outputs": [],
      "source": [
        "# Here I am going to create a dataframe in which we exclude the upper and lower quantiles from our numeric variable\n",
        "\n",
        "# Define filtering conditions\n",
        "lower_bound = 500 # I will assign this to be 500 based on the cell above\n",
        "upper_bound = df['price'].quantile(0.99) # 99th percentile\n",
        "\n",
        "df_filtered = df[\n",
        "    (df['price'] > lower_bound) & (df['price'] <= upper_bound) &  # Filter price\n",
        "    (df['year'] < df['year'].quantile(0.99)) & (df['year'] > df['year'].quantile(0.01)) &  # Filter year\n",
        "    (df['odometer'] < df['odometer'].quantile(0.99)) & (df['odometer'] > df['odometer'].quantile(0.01))  # Filter odometer\n",
        "]\n",
        "\n",
        "print(f'Price min: {df_filtered[\"price\"].min()}')\n",
        "print(f'Price max: {df_filtered[\"price\"].max()}')\n",
        "print(f'Year min: {df_filtered[\"year\"].min()}')\n",
        "print(f'Year max: {df_filtered[\"year\"].max()}')\n",
        "print(f'Odometer max: {df_filtered[\"odometer\"].max()}')\n",
        "print(f'Odometer min: {df_filtered[\"odometer\"].min()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M9tJyBRKL5w"
      },
      "outputs": [],
      "source": [
        "df_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmUK3EgNBZFx"
      },
      "outputs": [],
      "source": [
        "# Visualize missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_filtered.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
        "plt.title('Missing Data Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv2Pn8SjBZDr"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of null values for each feature\n",
        "null_percentage = df_filtered.isnull().sum() * 100 / len(df_filtered)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=null_percentage.index, y=null_percentage.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Percentage of Null Values')\n",
        "plt.title('Percentage of Null Values per Feature')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UiK4HPBZBk"
      },
      "outputs": [],
      "source": [
        "# I am going to drop some columns that either be will be erroneous to predicting price or have too high a percentage of null values.\n",
        "df_filtered = df_filtered.drop(columns=['size', 'VIN', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1covwGPBY7f"
      },
      "outputs": [],
      "source": [
        "df_filtered.dropna(subset=['year', 'price', 'odometer'], inplace=True) # Dropping nulls from our numeric features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDgqlqFrBY5Z"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of null values for each feature\n",
        "null_percentage = df_filtered.isnull().sum() * 100 / len(df_filtered)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=null_percentage.index, y=null_percentage.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Percentage of Null Values')\n",
        "plt.title('Percentage of Null Values per Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H379ory_unNt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXkecjSjBYt1"
      },
      "outputs": [],
      "source": [
        "# Removing rows with missing values in specific columns\n",
        "columns_to_remove_na = ['transmission', 'manufacturer', 'fuel', 'title_status', 'model']\n",
        "initial_rows = len(df_filtered)\n",
        "\n",
        "for col in columns_to_remove_na:\n",
        "    df_filtered = df_filtered[df_filtered[col].notna()]  # Remove rows where the column is NaN\n",
        "\n",
        "# Replacing NaN values with 'unknown' for categorical features\n",
        "columns_to_fill_unknown = ['condition', 'fuel','drive', 'type', 'paint_color', 'state']\n",
        "\n",
        "for col in columns_to_fill_unknown:\n",
        "    df_filtered[col] = df_filtered[col].fillna('unknown')\n",
        "    print(f\"{col} distinct values:\", df_filtered[col].unique(), \"\\n\")\n",
        "\n",
        "# Handling 'cylinders' column separately\n",
        "df_filtered['cylinders'] = df_filtered['cylinders'].fillna('unknown').replace('other', 'unknown')  # Replace NaN and \"other\"\n",
        "df_filtered = df_filtered[df_filtered['cylinders'] != 'unknown'].copy()  # Drop rows where cylinders = 'unknown'\n",
        "df_filtered['cylinders'] = df_filtered['cylinders'].str.replace('cylinders', '', regex=True).str.strip()  # Remove text\n",
        "df_filtered['cylinders'] = pd.to_numeric(df_filtered['cylinders'], errors='coerce')  # Convert to numeric (only care about number of cylinders)\n",
        "print(\"cylinders distinct values:\", df_filtered['cylinders'].unique(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gba8L_lrXUp"
      },
      "outputs": [],
      "source": [
        "# Make sure we have addressed all of the missing values.\n",
        "df_filtered.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pCc0PCQ6Ccg"
      },
      "source": [
        "The biggest challenge for me now is keeping my dimensionality in check. I need to one-hot-encode or oridnally-encode lots of the categorical features in my dataset. To simplify this, I am going to look at the frequencies of each of the categorical features and limit the unqiue values in each column to only those with the highest levels of frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIEFyFDLcyd_"
      },
      "outputs": [],
      "source": [
        "# Calculate average price per car color\n",
        "avg_price_by_color = df_filtered.groupby('paint_color')['price'].mean()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_price_by_color.index, y=avg_price_by_color.values)\n",
        "plt.xlabel(\"Car Color\")\n",
        "plt.ylabel(\"Average Price\")\n",
        "plt.title(\"Average Price of Cars by Color\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRfscj65cyXf"
      },
      "outputs": [],
      "source": [
        "# Calculate average price per transmission type\n",
        "avg_price_by_transmission = df_filtered.groupby('transmission')['price'].mean()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_price_by_transmission.index, y=avg_price_by_transmission.values)\n",
        "plt.xlabel(\"Transmission Type\")\n",
        "plt.ylabel(\"Average Price\")\n",
        "plt.title(\"Average Price of Cars by Transmission\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmhmr_8KcyPs"
      },
      "outputs": [],
      "source": [
        "# Calculate average price per title_status\n",
        "avg_price_by_title = df_filtered.groupby('title_status')['price'].mean()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_price_by_title.index, y=avg_price_by_title.values)\n",
        "plt.xlabel(\"Title Status\")\n",
        "plt.ylabel(\"Average Price\")\n",
        "plt.title(\"Average Price of Cars by Title Status\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMHwddyicyIr"
      },
      "outputs": [],
      "source": [
        "# Calculate average price per car type\n",
        "avg_price_by_type = df_filtered.groupby('type')['price'].mean()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_price_by_type.index, y=avg_price_by_type.values)\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Average Price\")\n",
        "plt.title(\"Average Price of Cars by Type\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq-FtZdicyBl"
      },
      "outputs": [],
      "source": [
        "df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOtjTurhdKwd"
      },
      "outputs": [],
      "source": [
        "clean_df = df_filtered.copy()\n",
        "# Delete df_filtered2 to save ram\n",
        "del df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUhKf2We4-9S"
      },
      "outputs": [],
      "source": [
        "# Apply the same filter to x and color\n",
        "odometer_filtered = clean_df[clean_df['odometer'] < 400000]\n",
        "\n",
        "# Now use the filtered DataFrame for all arguments\n",
        "fig = px.scatter(\n",
        "    x=clean_df['price'],\n",
        "    y=clean_df['odometer'],  # Also update y to use the filtered price\n",
        "    color=clean_df['price'],\n",
        "    title='Price vs. Odometer with Density',\n",
        "    labels = {'x':'Price', 'y':'Odometer'},\n",
        "    trendline='ols',\n",
        "    trendline_color_override='red'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgNr3UukA4VI"
      },
      "outputs": [],
      "source": [
        "# Now use the filtered DataFrame for all arguments\n",
        "fig = px.scatter(\n",
        "    x=clean_df['year'],\n",
        "    y=clean_df['odometer'],  # Also update y to use the filtered price\n",
        "    color=clean_df['price'],\n",
        "    title='Odometer vs. Year with Density',\n",
        "    labels = {'x':'Price', 'y':'Odometer'},\n",
        "    trendline='ols',\n",
        "    trendline_color_override='red'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx53Kpit3F9d"
      },
      "outputs": [],
      "source": [
        "# Now use the filtered DataFrame for all arguments\n",
        "fig = px.scatter(\n",
        "    x=clean_df['year'],\n",
        "    y=clean_df['price'],  # Also update y to use the filtered price\n",
        "    color=clean_df['price'],\n",
        "    title='Year vs. Price with Density',\n",
        "    labels = {'x':'Price', 'y':'Odometer'},\n",
        "    trendline='ols',\n",
        "    trendline_color_override='red'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAXl8jEAvrLB"
      },
      "source": [
        "### Modeling\n",
        "\n",
        "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNUS8kvkdKqQ"
      },
      "outputs": [],
      "source": [
        "# Begin by separating our numerical features.\n",
        "numerical_features = ['year','odometer','cylinders', 'price']\n",
        "numerical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGVlMQCz3s8K"
      },
      "outputs": [],
      "source": [
        "# Firstly we will look at some simple linear regressions with our numerical features.\n",
        "numeric_df = clean_df[numerical_features]\n",
        "numeric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag-WgNfT3s3L"
      },
      "outputs": [],
      "source": [
        "X_numeric = numeric_df.drop(columns=['price'])\n",
        "y_numeric = numeric_df['price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxR5CdA73s5j"
      },
      "outputs": [],
      "source": [
        "# Simple linear regression on year\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric[['year']], y_numeric, test_size=0.2, random_state=42)\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "y_pred = linreg.predict(X_test)\n",
        "mse_year_linreg = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "sorted_idx = X_test['year'].argsort()\n",
        "X_test_sorted = X_test.iloc[sorted_idx]\n",
        "y_pred_sorted = y_pred[sorted_idx]\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(X_test['year'], y_test, color='blue', label='Actual')\n",
        "plt.plot(X_test_sorted['year'], y_pred_sorted, color='red', linewidth=2, label='Prediction')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Linear Regression')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) for Linear Regression with 'Year' Feature: {np.sqrt(mse_year_linreg)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH7e2Nz43s0b"
      },
      "outputs": [],
      "source": [
        "# Simple linear regression on odometer\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric[['odometer']], y_numeric, test_size=0.2, random_state=42)\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "y_pred = linreg.predict(X_test)\n",
        "mse_odometer_linreg = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(X_test['odometer'], y_test, color='blue', label='Actual')\n",
        "plt.plot(X_test['odometer'], y_pred, color='red', linewidth=2, label='Prediction')\n",
        "plt.xlabel('Odometer')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Linear Regression of Odometer and Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) for Linear Regression with 'Odometer' Feature: {np.sqrt(mse_odometer_linreg)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dj7EHJw3sx2"
      },
      "outputs": [],
      "source": [
        "# Lets see if a multiple linear regression could improve our RMSE.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y_numeric, test_size=0.2, random_state=42)\n",
        "multiple_linreg = LinearRegression()\n",
        "multiple_linreg.fit(X_train, y_train)\n",
        "y_pred = multiple_linreg.predict(X_test)\n",
        "\n",
        "rmse_multiple_linreg = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Root Mean Squared Error (RMSE) for Multiple Linear Regression with numeric features: {rmse_multiple_linreg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RvnIYTu3su0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3EN8LNB3ssJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6YHH5avrLB"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPtLwMasvrLC"
      },
      "outputs": [],
      "source": [
        "# Comparing our RMSE scored for our different models.\n",
        "print(f\"RMSE for Linear Regression with 'Year' Feature: {np.sqrt(mse_year_linreg)}\")\n",
        "print(f\"RMSE for Linear Regression with 'Odometer' Feature: {np.sqrt(mse_odometer_linreg)}\")\n",
        "print(f\"RMSE for Multiple Linear Regression with numeric features: {rmse_multiple_linreg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6dQVFMbBuKX"
      },
      "source": [
        "The above cells shows us that we acheived the lowest RMSE score while using multiple linear regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wf7eni-vrLC"
      },
      "outputs": [],
      "source": [
        "# list of all the columns with a dtype == 'object' (isolates our categorical variables)\n",
        "object_columns = clean_df.select_dtypes(include=['object']).columns.tolist()\n",
        "object_columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df[object_columns].nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "I2ICsof6Yvwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to drop the model, region, and state features due to their high cardinality.\n",
        "clean_df = clean_df.drop(columns=['model', 'region', 'state'])"
      ],
      "metadata": {
        "id": "f7JLIKo9lTjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df[clean_df.select_dtypes(include=['object']).columns.tolist()].nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "EgRPKok7El_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making our dummy variables\n",
        "X_categorical = pd.get_dummies(clean_df, drop_first=True)\n",
        "y_categorical = clean_df['price']"
      ],
      "metadata": {
        "id": "LQiGpyd1loYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8OOE_sbvrLC"
      },
      "outputs": [],
      "source": [
        "# Standardizing\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKL2guoEvrLC"
      },
      "outputs": [],
      "source": [
        "# Separate Datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsGt7QLGP8zE"
      },
      "outputs": [],
      "source": [
        "# We are going to use PCA to help with dimensionality reduction\n",
        "pca = PCA(n_components=0.95) # Varience at 95% to balance between overfitting and over-simplicity\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx6eJU4TP8wP"
      },
      "outputs": [],
      "source": [
        "# Using Sequential Feature selection to limit our number of features.\n",
        "linreg_sfs = LinearRegression()\n",
        "selector = SequentialFeatureSelector(linreg_sfs, n_features_to_select=5, direction='forward')\n",
        "selector.fit(X_train_pca, y_train)\n",
        "X_train_sfs = selector.transform(X_train_pca)\n",
        "X_test_sfs = selector.transform(X_test_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1C7hvWZP8pG"
      },
      "outputs": [],
      "source": [
        "# Using GridSearchCV for optimizing Ridge and Lasso Regressions.\n",
        "\n",
        "# Building parameter grid\n",
        "parameter_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Creating GridSearchCV instances\n",
        "ridge = GridSearchCV(Ridge(), parameter_grid, cv=5)\n",
        "lasso = GridSearchCV(Lasso(), parameter_grid, cv=5)\n",
        "\n",
        "# Training models\n",
        "ridge.fit(X_train_sfs, y_train)\n",
        "lasso.fit(X_train_sfs, y_train)\n",
        "\n",
        "# Making predictions\n",
        "lasso_preds = lasso.predict(X_test_sfs)\n",
        "ridge_preds = ridge.predict(X_test_sfs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Performance of Ridge vs Lasso Models\n",
        "\n",
        "\n",
        "ridge_mse = mean_squared_error(y_test, ridge_preds) # Calculating MSE and RSME\n",
        "ridge_rsme = np.sqrt(ridge_mse)\n",
        "lasso_mse = mean_squared_error(y_test, lasso_preds)\n",
        "lasso_rsme = np.sqrt(lasso_mse)\n",
        "r2_lasso = r2_score(y_test, lasso_preds)\n",
        "r2_ridge = r2_score(y_test, ridge_preds)\n",
        "# Printing results\n",
        "print(f\"MSE for Ridge Regression: {ridge_mse}\")\n",
        "print(f\"MSE for Lasso Regression: {lasso_mse}\")\n",
        "print(f\"RMSE for Ridge Regression: {ridge_rsme}\")\n",
        "print(f\"RMSE for Lasso Regression: {lasso_rsme}\")\n",
        "print(f\"R2 for Ridge Regression: {r2_ridge}\")\n",
        "print(f\"R2 for Lasso Regression: {r2_lasso}\")"
      ],
      "metadata": {
        "id": "i2PC3LZGoNNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Side by side subplots comparing the Actual vs predicted for Lasso and Ridge\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,6)) # Figure with 1 row and 2 columns\n",
        "\n",
        "sns.regplot(x=y_test, y=lasso_preds, ax=ax[0], scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n",
        "ax[0].set_title('Lasso Regression')\n",
        "ax[0].set_xlabel('Actual Price')\n",
        "ax[0].set_ylabel('Predicted Price')\n",
        "\n",
        "sns.regplot(x=y_test, y=ridge_preds, ax=ax[1], scatter_kws={'color': 'blue'}, line_kws={'color': 'red'})\n",
        "ax[1].set_title('Ridge Regression')\n",
        "ax[1].set_xlabel('Actual Price')\n",
        "ax[1].set_ylabel('Predicted Price')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6uFvLWBWoNKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFY6_xAToNIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biZUSomLvrLC"
      },
      "source": [
        "### Deployment\n",
        "\n",
        "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lh513oZau781"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Sl1p1kvrLC"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for the visualization\n",
        "results_df = pd.DataFrame({'Actual Price': y_test, 'Predicted Price (Lasso)': lasso_preds})\n",
        "\n",
        "# Group data by year for mean values\n",
        "results_by_year = results_df.groupby(clean_df['year']).mean()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(results_by_year.index, results_by_year['Actual Price'], label='Actual Price', marker='o')\n",
        "plt.plot(results_by_year.index, results_by_year['Predicted Price (Lasso)'], label='Predicted Price (Lasso)', marker='x')\n",
        "\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Price')\n",
        "plt.title('Actual vs. Predicted Mean Price by Year')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tVVZHrqvrLC"
      },
      "outputs": [],
      "source": [
        "# Create the subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Subplot 1: Odometer vs. Price\n",
        "sns.regplot(x='odometer', y='price', data=clean_df, ax=axes[0], scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n",
        "axes[0].set_title('Odometer vs. Price')\n",
        "\n",
        "# Subplot 2: Year vs. Price\n",
        "sns.regplot(x='year', y='price', data=clean_df, ax=axes[1], scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n",
        "axes[1].set_title('Year vs. Price')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete dataframes to reduce file size\n",
        "\n",
        "# del clean_df\n",
        "# del results_df\n",
        "# del df_numeric\n",
        "# del df_no_outliers\n",
        "# del results_by_year\n",
        "# del odometer_filtered\n",
        "# del X_numeric\n",
        "# del y_numeric\n",
        "# del X_train\n",
        "# del X_test\n",
        "# del y_train\n",
        "# del y_test\n",
        "# del X_scaled\n",
        "# del X_categorical\n",
        "# del y_categorical\n",
        "# del X_train_pca\n",
        "# del X_test_pca\n",
        "# del X_train_sfs\n",
        "# del X_test_sfs\n",
        "# del linreg_sfs\n",
        "# del selector\n",
        "# del lasso\n",
        "# del ridge\n",
        "# del parameter_grid\n",
        "# del lasso_preds\n",
        "# del ridge_preds\n",
        "# del ridge_mse\n",
        "# del ridge_rsme\n",
        "# del lasso_mse\n",
        "# del lasso_rsme\n",
        "# del df\n",
        "\n"
      ],
      "metadata": {
        "id": "jdy0HOBUU-NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNjkmiwJBoBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}